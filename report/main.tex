\documentclass{article}
\nocite{*}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Selfie Tool for the Visually Impaired}
\author{Ethan Arnold, Alexander Cohen}

\begin{document}
\maketitle


\section{Development Process}
This project was pair-programmed, with us simultaneously using the computer.


For the project, we decided to use OpenCV for facial recognition.
We imported `cv2' and opened the camera as a video feed.
The facial detection returns a rectangle with a coordinate and dimensions.
We determine the position of the rectangle by finding the center of it, which is the position plus half the dimension respective to the axes.
The Quadrants of the page are determined by a range of values.
The video frame is a two-dimensional grid of pixels, so one can take the half of a dimension and use comparisons (greater than, less than) to determine if it is on the top/bottom or left/right side of the screen.
Compounding this idea with both axes, you can then determine if the rectangle is in one of four quadrants.


For Text-To-Speech, we used `pyttsx3'. We specifically picked this library because it does not require network connection.
This means there are less `weak links' in the chain to rely on.
The text-To-Speech is ran every time the quadrant changes.
The change is detected by comparing the current frame's quadrant to the previous.
This prevents the TTS from enabling every frame.
This would especially be a problem, as we did not fully understand how to multithread. This will be later discussed in 'Potential Improvements.'

For Speech Input, we multithreaded the speech input.
Sometimes, the computer mistakes words for homophones and similar phonemes, so this was accounted for.
The speech input thread loops until the program exits. It saves what was thought to be said as a string and sends the string off to be interpreted.
There is a function with a match that matches the string to another function to do an action.
Photographing, exiting, and telling the computer which coordinate are possible functions.

\section{Usage Statistics}

We did two tests on anonymous people and got timing results.
For the first test, we had the subject start out of the frame and command the computer to look in the top left.
The first person took 36 seconds, while the second 26 seconds.
The second test, the user started in the bottom right quadrant and moves to the top left.
For the second test, the first person took 26 seconds while the latter took 42 seconds.
There was a larger gap in time, which most probably was caused by facial detection malfunctioning.


\section{Similar Applications}
A program such as ours could have many other practicle applications. 
Unfortunatly, people who have impaired vision sometimes struggle to do tasks indepedently or even at all. 
For example, goggles could be built with computer vision technolagy that would identify objects in the way of ones path.
Effectively nullifying the need for walking sticks or help from others. 
A similar application could give the visually impaired the ability to read and write.
Tools such as these could give the visually impared much more independence. 
\section{Potential Improvement}


There were some glitches when there were multiple heads in the frame.
This could be remedied by selecting one head as the main head.
The parameters for selecting a head could be determined by the size of the head (closest human).


We need to learn proper multithreading, because when the user's head switches quadrants, the video freezes until the Text-To-Speech ends.
In order to fix this, check every frame if there is a Text-To-Speech thread running, and if not, run.


Another issue is that the Speech Recognition runs while the program also speaks.
This creates a problem if the Text-To-Speech phrase exists as a command.
Luckily, for us, this was not the case, but if were, we would need to implement the aforementioned check in the previous paragraph.


The GUI was default from the OpenCV, which could have been reimplemented with the QT framework to make a more robust UI.


\bibliographystyle{alpha}

\end{document}